---
title: "Neuronales Netz"
output: html_notebook
author: C. Aurich
---

### Vorbereitung der Umgebung ###
```{r}
# Umgebungsvariablen löschen
remove(list = ls())
# Einbinden benötigter Funktionsbibliotheken
library(reticulate)
library(readr)
library(fastDummies)
library(ggplot2)
library(Metrics)

# Funktionsdefinitionen zur späteren Produktion standartisierter Werte
# (übernommen aus dem Data-Science-Kurs-Skript vom 21.01.2020)
norm_cols <- function (.data, norm_values = NULL) {
  for (i in 1:nrow(norm_values)  ) {
    .data$norm <- (.data[[norm_values$name[i]]] - norm_values$mean[i]) / norm_values$sd[i]
    names(.data)[length(.data)] <- paste0(norm_values$name[i], "_norm")
  }
  return (.data)
}

#' Title Creation of a Dataframe including the Information to Standardize Variables
#' This function is meant to be used in combination with the function norm_cols
get.norm_values <- function (.data, select_columns = NULL) {
  result <- NULL
  for (col_name in select_columns) {
    mean <- mean(.data[[col_name]], na.rm = TRUE)
    sd <- sd(.data[[col_name]], na.rm = TRUE)
    result <- rbind (result, c(mean, sd))
  }
  result <- as.data.frame(result, stringsAsFactors = FALSE)
  result <- data.frame (select_columns, result, stringsAsFactors = FALSE)
  names(result) <- c("name", "mean", "sd")
  return (result)
}
```

### Aufbereitung der Daten ###
```{r}
# Einlesen der Trainingsdaten
training_df <- read_csv("train.csv")
# Anpassen der Datenformate
training_df$Datum <- as.Date(training_df$Datum, format = "%Y-%m-%d")
#training_df$Temperaturklassen <- as.factor(training_df$Temperaturklassen)
#X1 Spalte loeschen
training_df$X1<-NULL
```


```{r}
# Rekodierung von kategoriellen Variablen (zu Dummy-Variablen)
dummy_list <- c("Wochentag", "Monat", "Temperaturklassen")
training_df_dummies <- dummy_cols(training_df, dummy_list)
# Entfernen an dieser Stelle nicht weiter benötigter Spalten
training_df_dummies$Wochentag<-NULL
training_df_dummies$Monat<-NULL
training_df_dummies$Temperaturklassen<-NULL
training_df_dummies$X1<-NULL

#Speichern der rekodierten Dummytrainingsdaten
write_csv(training_df_dummies, "training_df_dummies.csv")

# Standardisierung von metrischen Variablen
norm_list <- c("Wochentag", "Monat", "Temperaturklassen")
# Berechnung der Mittelwerte und Standardabweichungen der zu standardisierenden Variablen
norm_values_list <- get.norm_values(training_df_dummies, norm_list)
# Standardisierung der angegebenen metrischen Variablen
training_df_norm <- norm_cols(training_df_dummies, norm_values_list)
# Definition von Variablenlisten, um das Arbeiten mit diesen zu erleichtern
Wochentag_dummies = c('Wochentag_1', 'Wochentag_2', 'Wochentag_3', 'Wochentag_4', 'Wochentag_5', 'Wochentag_6', 'Wochentag_7')
Monat_dummies = c('Monat_1', 'Monat_2', 'Monat_3', 'Monat_4', 'Monat_5', 'Monat_6', 'Monat_7', 'Monat_8', 'Monat_9', 'Monat_10', 'Monat_11', 'Monat_12')
Temperaturklassen_dummies = c('Temperaturklassen_kalt', 'Temperaturklassen_normal', 'Temperaturklassen_warm')
# Definition der Features (der unabhängigen Variablen auf deren Basis die Vorhersagen erzeugt werden sollen)
features = c('sqft_lot_norm', 'condition_norm', 'grade_norm', 'bathrooms_norm', view_dummies, waterfront_dummies)
# Definition der Label-Variable (der abhaengigen Variable, die vorhergesagt werden soll) sowie
label = 'price_norm'
# Zufallszähler setzen, um die zufällige Partitionierung bei jedem Durchlauf gleich zu halten
set.seed(42)
# Bestimmung der Indizes des Traininsdatensatzes
train_ind <- sample(seq_len(nrow(house_pricing_norm)), size = floor(0.80 * nrow(house_pricing_norm)))
# Teilen in Trainings- und Testdatensatz
train_dataset = house_pricing_norm[train_ind, features]
test_dataset = house_pricing_norm[-train_ind, features]
# Selektion der Variable, die als Label definiert wurde
train_labels = house_pricing_norm[train_ind, label]
test_labels = house_pricing_norm[-train_ind, label]
```


VON STEFFEN!!
```{r}
# Rekodierung von kategoriellen Variablen (zu Dummy-Variablen)
dummy_list <- c("view", "waterfront")
house_pricing_dummy = dummy_cols(house_pricing, dummy_list)
# Standardisierung von metrischen Variablen
norm_list <- c("price", "sqft_lot", "bathrooms", "grade", "condition")
# Berechnung der Mittelwerte und Standardabweichungen der zu standardisierenden Variablen
norm_values_list <- get.norm_values(house_pricing_dummy, norm_list)
# Standardisierung der angegebenen metrischen Variablen
house_pricing_norm <- norm_cols(house_pricing_dummy, norm_values_list)
# Definition von Variablenlisten, um das Arbeiten mit diesen zu erleichtern
waterfront_dummies = c('waterfront_0', 'waterfront_1')
view_dummies = c('view_0', 'view_1', 'view_2', 'view_3','view_4')
# Definition der Features (der unabhängigen Variablen auf deren Basis die Vorhersagen erzeugt werden sollen)
features = c('sqft_lot_norm', 'condition_norm', 'grade_norm', 'bathrooms_norm', view_dummies, waterfront_dummies)
# Definition der Label-Variable (der abhaengigen Variable, die vorhergesagt werden soll) sowie
label = 'price_norm'
# Zufallszähler setzen, um die zufällige Partitionierung bei jedem Durchlauf gleich zu halten
set.seed(1)
# Bestimmung der Indizes des Traininsdatensatzes
train_ind <- sample(seq_len(nrow(house_pricing_norm)), size = floor(0.80 * nrow(house_pricing_norm)))
# Teilen in Trainings- und Testdatensatz
train_dataset = house_pricing_norm[train_ind, features]
test_dataset = house_pricing_norm[-train_ind, features]
# Selektion der Variable, die als Label definiert wurde
train_labels = house_pricing_norm[train_ind, label]
test_labels = house_pricing_norm[-train_ind, label]
```


### Schätzung des Neuronalen Netzes
```{python}
# Benoetigte Python Libraries einbinden
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
# Definition der Form des tiefen neuronalen Netzes (Deep Neural Nets)
model = keras.Sequential([
  layers.Dense(5, activation='relu', input_shape=[len(r.training_df_dummies.keys())]),
  layers.Dense(4, activation='relu'),
  layers.Dense(1)
])
# Definition der Kosten-(Loss-)Funktion und der Optimierungsfunktion mit seinen Hyperparametern
model.compile(loss="mse",
              optimizer=tf.keras.optimizers.SGD(lr=1e-6, momentum=0.9))
# Ausgabe einer Zusammenfassung zur Form des Modells, das geschaetzt wird (nicht notwendig)
model.summary()
# Schaetzung des Modells
history = model.fit(r.train_dataset, r.train_labels, epochs=2, validation_split = 0.1, verbose=0)
```


### Speichern des Neuronalen Netzes für spätere Vorhersagen ###
```{python}
model.save("python_model.h5")
```


### Auswertung der Modelloptimierung ###
```{r}
# Grafische Ausgabe der Modelloptimierung
# create data
data <- data.frame(val_loss = unlist(py$history$history$val_loss),
                  loss = unlist(py$history$history$loss))
# Plot
ggplot(data[-1,]) +
  geom_line( aes(x=1:length(val_loss), y=val_loss, colour = "Validation Loss" )) +
  geom_line( aes(x=1:length(loss), y=loss, colour = "Training Loss" )) +
  scale_colour_manual( values = c("Training Loss"="blue", "Validation Loss"="red") ) +
  labs(title="Loss Function Values During Optimization") +
  xlab("Iteration Number") +
  ylab("Loss") 
```


### Laden eines gespeicherten Neuronalen Netzes ###
```{python}
model = keras.models.load_model("python_model_best.h5")
```

### Auswertung der Schätzergebnisse ###
```{r}
# Schätzung der (normierten) Preise für die Trainings- und Testdaten
train_predictions_norm <- py$model$predict(train_dataset)
test_predictions_norm <- py$model$predict(test_dataset)
# Rückberechnung der normierten Preisschätzungen zu den tatsächlichen Preisschätzungen bzw. Preisen
train_predictions <- (train_predictions_norm * norm_values_list$sd[1] ) + norm_values_list$mean[1]
test_predictions <- (test_predictions_norm * norm_values_list$sd[1]) + norm_values_list$mean[1]
# Selektion der zugehörigen tatsächlichen Preise
train_actuals <- house_pricing$price[train_ind]
test_actuals <- house_pricing$price[-train_ind]
# Vergleich der Gütekriterien für die Traingings- und Testdaten
cat(paste0("MAPE on the Training Data:\t", format(mape(train_actuals, train_predictions)*100, digits=3, nsmall=2)))
cat(paste0("\nMAPE on the Test Data:\t\t", format(mape(test_actuals, test_predictions)*100, digits=3, nsmall=2)))
```

```{r}
## Grafischer vergleich der vorhergesagten und der tatsächlichen Preise für die Trainings- und Testdaten
# Zusammenstellung der Daten für die Plots
data_train <- data.frame(prediction = train_predictions/1000, actual = train_actuals/1000)
data_test <- data.frame(prediction = test_predictions/1000, actual = test_actuals/1000)
# Plot der Ergebnisse der Trainingsdaten
ggplot(data_train[1:100,]) +
  geom_line( aes(x=1:length(prediction), y=prediction, colour = "Predicted Values" )) +
  geom_line( aes(x=1:length(actual), y=actual, colour = "Actual Values" )) +
  scale_colour_manual( values = c("Predicted Values"="blue", "Actual Values"="red") ) +
  labs(title="Predicted and Actual Values for the Training Data") +
  xlab("Case Number") +
  ylab("Price in 1.000 USD") 
# Plot der Ergebnisse der Testdaten
ggplot(data_test[1:100,]) +
  geom_line( aes(x=1:length(prediction), y=prediction, colour = "Predicted Values" )) +
  geom_line( aes(x=1:length(actual), y=actual, colour = "Actual Values" )) +
  scale_colour_manual( values = c("Predicted Values"="blue", "Actual Values"="red") ) +
  labs(title="Predicted and Actual Values for the Test Data") +
  xlab("Case Number") +
  ylab("Price in 1.000 USD") 
```

```{r}
# Vorhersage für einen einzelnen Fall
cat(paste0("Vorergesagter Preis:\t", format(test_predictions[100], digits=2, nsmall =0)))
cat(paste0("\nTatsächlicher Preis:\t", test_actuals[100]))
```